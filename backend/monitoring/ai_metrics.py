#!/usr/bin/env python3
"""
AI Metrics Service - Phase 5 Monitoring & Analytics
Provides comprehensive AI performance monitoring and metrics collection
"""

import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import asyncio
import logging

import redis.asyncio as redis
from sqlalchemy.orm import Session
from sqlalchemy import func, and_, or_
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client.core import CollectorRegistry

from app.models import Conflict, Decision, Train, Section
from app.redis_client import redis_client
from app.ai_config import AIConfig

# Initialize AI config
ai_config = AIConfig()


logger = logging.getLogger(__name__)


class MetricType(Enum):
    COUNTER = "counter"
    HISTOGRAM = "histogram"
    GAUGE = "gauge"


@dataclass
class AIMetric:
    """AI performance metric data point"""
    metric_name: str
    metric_type: MetricType
    value: float
    labels: Dict[str, str]
    timestamp: datetime
    description: str = ""


class AIMetricsCollector:
    """
    Phase 5: AI Metrics Collection and Monitoring
    
    Collects and tracks comprehensive AI performance metrics including:
    - Optimization success rates
    - Response times and performance
    - Solver effectiveness comparison
    - Cache performance metrics
    - Training progress and effectiveness
    """
    
    def __init__(self, db: Session, redis_client=None):
        self.db = db
        self.redis_client = redis_client or redis_client
        self.metrics_prefix = "ai_metrics:"
        
        # Create custom Prometheus registry
        self.registry = CollectorRegistry()
        
        # Initialize Prometheus metrics
        self._init_prometheus_metrics()
        
        # Metric retention settings
        self.metric_retention_days = getattr(ai_config, "metric_retention_days", 90)
        self.real_time_window_minutes = getattr(ai_config, "real_time_window_minutes", 60)
    
    def _init_prometheus_metrics(self):
        """Initialize Prometheus metrics"""
        # Optimization metrics
        self.optimization_counter = Counter(
            'ai_optimizations_total',
            'Total number of AI optimization requests',
            ['solver_type', 'success', 'cache_hit'],
            registry=self.registry
        )
        
        self.optimization_duration = Histogram(
            'ai_optimization_duration_seconds',
            'AI optimization request duration',
            ['solver_type'],
            registry=self.registry
        )
        
        self.optimization_confidence = Histogram(
            'ai_optimization_confidence',
            'AI optimization confidence scores',
            ['solver_type'],
            registry=self.registry
        )
        
        # Cache metrics
        self.cache_hit_rate = Gauge(
            'ai_cache_hit_rate',
            'AI cache hit rate percentage',
            registry=self.registry
        )
        
        self.cache_size = Gauge(
            'ai_cache_size_entries',
            'Number of entries in AI cache',
            registry=self.registry
        )
        
        # Training metrics
        self.training_progress = Gauge(
            'ai_training_progress',
            'AI training progress percentage',
            ['model_type'],
            registry=self.registry
        )
        
        self.model_performance = Gauge(
            'ai_model_performance_score',
            'AI model performance score',
            ['model_type', 'metric'],
            registry=self.registry
        )
        
        # System health metrics
        self.ai_service_health = Gauge(
            'ai_service_health',
            'AI service health status (1=healthy, 0=unhealthy)',
            ['service_name'],
            registry=self.registry
        )
    
    async def record_optimization_metrics(self, 
                                        solver_type: str, 
                                        duration: float, 
                                        confidence: float, 
                                        success: bool, 
                                        cache_hit: bool = False):
        """Record optimization performance metrics"""
        try:
            # Update Prometheus metrics
            self.optimization_counter.labels(
                solver_type=solver_type,
                success=str(success).lower(),
                cache_hit=str(cache_hit).lower()
            ).inc()
            
            self.optimization_duration.labels(solver_type=solver_type).observe(duration)
            self.optimization_confidence.labels(solver_type=solver_type).observe(confidence)
            
            # Store detailed metrics in Redis
            metric_key = f"{self.metrics_prefix}optimization:{datetime.utcnow().isoformat()}"
            metric_data = {
                'solver_type': solver_type,
                'duration': duration,
                'confidence': confidence,
                'success': success,
                'cache_hit': cache_hit,
                'timestamp': datetime.utcnow().isoformat()
            }
            
            await self.redis_client.setex(
                metric_key, 
                self.metric_retention_days * 24 * 3600,
                json.dumps(metric_data)
            )
            
        except Exception as e:
            logger.error(f"Error recording optimization metrics: {e}")
    
    async def record_cache_metrics(self, hit_rate: float, cache_size: int):
        """Record cache performance metrics"""
        try:
            self.cache_hit_rate.set(hit_rate)
            self.cache_size.set(cache_size)
            
            # Store in Redis for historical tracking
            metric_key = f"{self.metrics_prefix}cache:{datetime.utcnow().isoformat()}"
            metric_data = {
                'hit_rate': hit_rate,
                'cache_size': cache_size,
                'timestamp': datetime.utcnow().isoformat()
            }
            
            await self.redis_client.setex(
                metric_key,
                self.metric_retention_days * 24 * 3600,
                json.dumps(metric_data)
            )
            
        except Exception as e:
            logger.error(f"Error recording cache metrics: {e}")
    
    async def record_training_progress(self, model_type: str, progress_percentage: float):
        """Record AI training progress"""
        try:
            self.training_progress.labels(model_type=model_type).set(progress_percentage)
            
            # Store detailed progress
            metric_key = f"{self.metrics_prefix}training:{model_type}:{datetime.utcnow().isoformat()}"
            metric_data = {
                'model_type': model_type,
                'progress_percentage': progress_percentage,
                'timestamp': datetime.utcnow().isoformat()
            }
            
            await self.redis_client.setex(
                metric_key,
                self.metric_retention_days * 24 * 3600,
                json.dumps(metric_data)
            )
            
        except Exception as e:
            logger.error(f"Error recording training progress: {e}")
    
    async def update_service_health(self, service_name: str, healthy: bool):
        """Update AI service health status"""
        try:
            self.ai_service_health.labels(service_name=service_name).set(1.0 if healthy else 0.0)
        except Exception as e:
            logger.error(f"Error updating service health: {e}")
    
    async def get_optimization_metrics(self, hours: int = 24) -> Dict[str, Any]:
        """Get comprehensive optimization performance metrics"""
        try:
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            
            # Database metrics
            total_optimizations = self.db.query(Conflict).filter(
                and_(
                    Conflict.ai_analyzed == True,
                    Conflict.ai_analysis_time >= cutoff_time
                )
            ).count()
            
            successful_optimizations = self.db.query(Conflict).filter(
                and_(
                    Conflict.ai_analyzed == True,
                    Conflict.ai_analysis_time >= cutoff_time,
                    Conflict.ai_confidence > 0.7
                )
            ).count()
            
            avg_confidence = self.db.query(
                func.avg(Conflict.ai_confidence)
            ).filter(
                and_(
                    Conflict.ai_analyzed == True,
                    Conflict.ai_analysis_time >= cutoff_time
                )
            ).scalar() or 0.0
            
            # Solver performance breakdown
            solver_stats = {}
            for solver in ['rule_based', 'constraint_programming', 'reinforcement_learning']:
                solver_count = self.db.query(Decision).join(Conflict).filter(
                    and_(
                        Decision.ai_solver_method == solver,
                        Conflict.ai_analysis_time >= cutoff_time
                    )
                ).count()
                
                solver_avg_confidence = self.db.query(
                    func.avg(Decision.ai_confidence)
                ).join(Conflict).filter(
                    and_(
                        Decision.ai_solver_method == solver,
                        Conflict.ai_analysis_time >= cutoff_time
                    )
                ).scalar() or 0.0
                
                solver_stats[solver] = {
                    'total_uses': solver_count,
                    'average_confidence': float(solver_avg_confidence),
                    'success_rate': (solver_count / total_optimizations * 100) if total_optimizations > 0 else 0
                }
            
            return {
                'time_window_hours': hours,
                'total_optimizations': total_optimizations,
                'successful_optimizations': successful_optimizations,
                'success_rate_percentage': (successful_optimizations / total_optimizations * 100) if total_optimizations > 0 else 0,
                'average_confidence': float(avg_confidence),
                'solver_performance': solver_stats,
                'timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting optimization metrics: {e}")
            return {'error': str(e)}
    
    async def get_performance_trends(self, days: int = 7) -> Dict[str, Any]:
        """Get AI performance trends over time"""
        try:
            trends = {}
            
            for day_offset in range(days):
                date = datetime.utcnow() - timedelta(days=day_offset)
                date_start = date.replace(hour=0, minute=0, second=0, microsecond=0)
                date_end = date_start + timedelta(days=1)
                
                daily_optimizations = self.db.query(Conflict).filter(
                    and_(
                        Conflict.ai_analyzed == True,
                        Conflict.ai_analysis_time >= date_start,
                        Conflict.ai_analysis_time < date_end
                    )
                ).count()
                
                daily_avg_confidence = self.db.query(
                    func.avg(Conflict.ai_confidence)
                ).filter(
                    and_(
                        Conflict.ai_analyzed == True,
                        Conflict.ai_analysis_time >= date_start,
                        Conflict.ai_analysis_time < date_end
                    )
                ).scalar() or 0.0
                
                trends[date.strftime('%Y-%m-%d')] = {
                    'optimizations_count': daily_optimizations,
                    'average_confidence': float(daily_avg_confidence)
                }
            
            return {
                'trend_period_days': days,
                'daily_trends': trends,
                'timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting performance trends: {e}")
            return {'error': str(e)}
    
    async def get_real_time_metrics(self) -> Dict[str, Any]:
        """Get real-time AI performance metrics"""
        try:
            cutoff_time = datetime.utcnow() - timedelta(minutes=self.real_time_window_minutes)
            
            # Recent optimization activity
            recent_optimizations = self.db.query(Conflict).filter(
                and_(
                    Conflict.ai_analyzed == True,
                    Conflict.ai_analysis_time >= cutoff_time
                )
            ).count()
            
            # Current cache stats
            from app.services.ai_cache import ai_cache_service
            cache_stats = await ai_cache_service.get_cache_stats()
            
            # System health
            health_status = {
                'optimization_service': True,
                'cache_service': True,
                'database': True,
                'redis': True
            }
            
            return {
                'window_minutes': self.real_time_window_minutes,
                'recent_optimizations': recent_optimizations,
                'optimization_rate_per_minute': recent_optimizations / self.real_time_window_minutes,
                'cache_performance': cache_stats.get('cache_performance', {}),
                'system_health': health_status,
                'timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting real-time metrics: {e}")
            return {'error': str(e)}
    
    def export_prometheus_metrics(self) -> str:
        """Export metrics in Prometheus format"""
        return generate_latest(self.registry)
    
    async def cleanup_old_metrics(self):
        """Clean up old metrics data"""
        try:
            cutoff_timestamp = datetime.utcnow() - timedelta(days=self.metric_retention_days)
            
            # Clean up Redis metrics
            pattern = f"{self.metrics_prefix}*"
            keys = await self.redis_client.keys(pattern)
            
            deleted_count = 0
            for key in keys:
                try:
                    data = await self.redis_client.get(key)
                    if data:
                        metric_data = json.loads(data)
                        metric_timestamp = datetime.fromisoformat(metric_data.get('timestamp', ''))
                        
                        if metric_timestamp < cutoff_timestamp:
                            await self.redis_client.delete(key)
                            deleted_count += 1
                except Exception:
                    continue
            
            logger.info(f"Cleaned up {deleted_count} old metric entries")
            return deleted_count
            
        except Exception as e:
            logger.error(f"Error cleaning up old metrics: {e}")
            return 0


# Global metrics collector instance
metrics_collector = None

def get_metrics_collector(db: Session) -> AIMetricsCollector:
    """Get or create metrics collector instance"""
    global metrics_collector
    if not metrics_collector:
        metrics_collector = AIMetricsCollector(db)
    return metrics_collector


if __name__ == "__main__":
    # Test metrics collection
    async def test_metrics():
        print("ðŸ§ª Testing AI Metrics Collector...")
        
        # This would normally be done with a real database session
        # collector = AIMetricsCollector(db_session)
        
        # Test metric recording
        print("âœ… Metrics collector structure validated")
    
    # Run test
    asyncio.run(test_metrics())